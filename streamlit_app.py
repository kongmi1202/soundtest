import streamlit as st
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import pretty_midi
import io
from pathlib import Path
import tempfile

st.set_page_config(page_title='AI ìŒ¤ ê°€ì°½ ì½”ì¹˜', page_icon='ğŸ¶')

st.title("AI ìŒ¤ê³¼ í•¨ê»˜í•˜ëŠ” ë”©ë™ëŒ• ê°€ì°½ ì½”ì¹˜")
st.markdown(
    """
    í•™ìƒë“¤ì´ ì—…ë¡œë“œí•œ ê°€ì°½ ë…¹ìŒ(MP3, WAV, M4A ë“±)ì„ ì •ë‹µ MIDI(ì„ íƒ)ì™€ ë¹„êµí•˜ì—¬
    ìŒì •(pitch)Â·ë°•ì(timing) ì •í™•ë„ë¥¼ ì‹œê°í™”í•˜ê³  ê°„ë‹¨í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

    ì‚¬ìš©ë²•:
    1) í•™ìƒ ë…¹ìŒ íŒŒì¼ ì—…ë¡œë“œ
    2) (ì„ íƒ) ì •ë‹µ MIDI ì—…ë¡œë“œ â€” ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œëœ ë©œë¡œë””ë¥¼ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©
    3) ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²°ê³¼ í™•ì¸
    """
)

st.sidebar.header("ê¸°ëŠ¥ ìš”ì•½")
st.sidebar.markdown(
    "- ìŒì • ì¶”ì : librosa.pyin (ì—†ìœ¼ë©´ yin) ì‚¬ìš©\n"
    "- ë°•ì(ì˜¨ì…‹) ë¹„êµ: librosa.onset & MIDI onset ë¹„êµ\n"
    "- ê°„ë‹¨í•œ ì •í™•ë„ ì ìˆ˜(ìŒì •/ë°•ì) ë° í”¼ì¹˜ íŠ¸ë™ ì‹œê°í™”\n\n"
    "í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬: librosa, pretty_midi, matplotlib"
)

uploaded_audio = st.file_uploader("í•™ìƒ ë…¹ìŒ íŒŒì¼ ì—…ë¡œë“œ (MP3, WAV, M4A)", type=["mp3", "wav", "m4a"])
uploaded_midi = st.file_uploader("ì •ë‹µ MIDI íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ)", type=["mid", "midi"])

# ì¬ìƒ ê¸°ëŠ¥: ì—…ë¡œë“œ í›„ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì¬ìƒ
if uploaded_audio:
    st.caption(f"ì—…ë¡œë“œëœ íŒŒì¼: {uploaded_audio.name}")
    st.audio(uploaded_audio)  # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì¬ìƒ

threshold_cents = st.slider("ìŒì • ì •í™•ë„ í—ˆìš© ë²”ìœ„ (cent)", min_value=10, max_value=200, value=50, step=5)
onset_tolerance = st.slider("ë°•ì(ì˜¨ì…‹) í—ˆìš© ì˜¤ì°¨ (ì´ˆ)", min_value=0.01, max_value=0.5, value=0.12, step=0.01)

@st.cache_data
def load_audio_file(file_bytes):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        # librosa can usually read mp3/m4a via soundfile/audioread; ensure a stable path
        tmp.write(file_bytes.getvalue())
        tmp_path = tmp.name
    y, sr = librosa.load(tmp_path, sr=22050, mono=True)
    return y, sr

@st.cache_data
def extract_pitch(y, sr):
    # Try pYIN first (librosa.pyin), fallback to yin
    try:
        f0, voiced_flag, voiced_prob = librosa.pyin(y, fmin=librosa.note_to_hz('C2'),
                                                    fmax=librosa.note_to_hz('C7'))
    except Exception:
        # fallback
        f0 = librosa.yin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))
        voiced_flag = ~np.isnan(f0)
    times = librosa.times_like(f0, sr=sr)
    return times, f0, voiced_flag

@st.cache_data
def midi_to_reference(midi_bytes, sr=22050):
    pm = pretty_midi.PrettyMIDI(io.BytesIO(midi_bytes.getvalue()))
    # Create a time grid and synthesize reference pitch (monophonic melody: highest piano roll per frame)
    duration = pm.get_end_time()
    times = np.linspace(0, duration, int(duration * sr // 256) + 1)  # coarse grid
    ref_freq = np.full_like(times, np.nan, dtype=float)
    for inst in pm.instruments:
        for note in inst.notes:
            idx = np.where((times >= note.start) & (times <= note.end))
            ref_freq[idx] = pretty_midi.note_number_to_hz(note.pitch)
    # If no MIDI notes found, return empty
    return times, ref_freq

def freq_to_cents(f_a, f_b):
    return 1200 * np.log2(f_a / f_b)

def compute_pitch_accuracy(student_times, student_f0, ref_times, ref_f0, cents_thresh):
    # Interpolate reference to student times
    ref_interp = np.interp(student_times, ref_times, np.nan_to_num(ref_f0, nan=0.0))
    mask_voiced = ~np.isnan(student_f0) & (student_f0 > 0) & (ref_interp > 0)
    if mask_voiced.sum() == 0:
        return 0.0, []
    cents_diff = np.abs(1200 * np.log2(student_f0[mask_voiced] / ref_interp[mask_voiced]))
    accurate = cents_diff <= cents_thresh
    accuracy = accurate.sum() / len(accurate) * 100.0
    # return accuracy and list of (time, cents_diff) for voiced frames
    times = student_times[mask_voiced]
    return accuracy, list(zip(times.tolist(), cents_diff.tolist(), accurate.tolist()))

def compute_onset_accuracy(y, sr, ref_onsets, tol):
    # detect onsets in student's audio
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='time', backtrack=False)
    if len(ref_onsets) == 0:
        return 0.0, onset_frames, ref_onsets
    matched = 0
    used_ref = []
    for ro in ref_onsets:
        # if any detected onset within tol of ro
        if np.any(np.abs(onset_frames - ro) <= tol):
            matched += 1
            used_ref.append(ro)
    accuracy = matched / len(ref_onsets) * 100.0
    return accuracy, onset_frames, np.array(used_ref)

def midi_onsets_from_midi(pm):
    onsets = []
    for inst in pm.instruments:
        for note in inst.notes:
            onsets.append(note.start)
    return np.array(sorted(onsets))

# Main action
if st.button("ë¶„ì„ ì‹œì‘"):

    if not uploaded_audio:
        st.warning("ë¨¼ì € í•™ìƒ ë…¹ìŒ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ì˜¤ë””ì˜¤ ë¡œë“œ ë° í”¼ì¹˜ ì¶”ì¶œ ì¤‘..."):
        y, sr = load_audio_file(uploaded_audio)
        student_times, student_f0, voiced_flag = extract_pitch(y, sr)

    ref_times = None
    ref_f0 = None
    ref_onsets = np.array([])

    if uploaded_midi:
        try:
            pm = pretty_midi.PrettyMIDI(io.BytesIO(uploaded_midi.getvalue()))
            ref_onsets = midi_onsets_from_midi(pm)
            # create ref time grid matching student_times for easier plotting/comparison
            ref_times = student_times
            ref_f0 = np.full_like(ref_times, np.nan, dtype=float)
            for inst in pm.instruments:
                for note in inst.notes:
                    idx = (ref_times >= note.start) & (ref_times <= note.end)
                    ref_f0[idx] = pretty_midi.note_number_to_hz(note.pitch)
        except Exception as e:
            st.warning(f"MIDI íŒŒì‹± ì‹¤íŒ¨: {e}. ìë™ ì¶”ì¶œ ë©œë¡œë””ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            uploaded_midi = None

    if not uploaded_midi:
        # Try simple harmonic/percussive separation + librosa.harmonic -> pitch as ref
        onset_env = librosa.onset.onset_strength(y=y, sr=sr)
        # use student's harmonic component as approximate reference (useful when no MIDI)
        y_harmonic = librosa.effects.harmonic(y)
        ref_times_alt, ref_f0_alt, _ = extract_pitch(y_harmonic, sr)
        ref_times = ref_times_alt
        ref_f0 = ref_f0_alt
        ref_onsets = librosa.onset.onset_detect(y=y, sr=sr, units='time')

    # Compute pitch accuracy
    pitch_acc, pitch_details = compute_pitch_accuracy(student_times, student_f0, ref_times, ref_f0, threshold_cents)

    # Compute onset/timing accuracy
    onset_acc, detected_onsets, matched_ref_onsets = compute_onset_accuracy(y, sr, ref_onsets, onset_tolerance)

    # Summary
    st.subheader("ìš”ì•½ ê²°ê³¼")
    st.metric("ìŒì • ì •í™•ë„", f"{pitch_acc:.1f} %")
    st.metric("ë°•ì(ì˜¨ì…‹) ì •í™•ë„", f"{onset_acc:.1f} %")

    # Highlight worst segments (lowest accuracy frames)
    if pitch_details:
        # pitch_details: list of (time, cents_diff, accurate_bool)
        bad = [d for d in pitch_details if not d[2]]
        if bad:
            # show up to 3 worst by cents diff
            bad_sorted = sorted(bad, key=lambda x: -x[1])[:3]
            st.markdown("#### ì·¨ì•½ êµ¬ê°„(ìŒì •)")
            for t, cents, _ in bad_sorted:
                st.write(f"- {t:.2f}s â€” í¸ì°¨ {cents:.1f} cent (ê¶Œì¥ ì—°ìŠµ: í•´ë‹¹ êµ¬ì—­ì˜ ëŠë¦° í…œí¬ ë°˜ë³µ ì—°ìŠµ)")
        else:
            st.write("ìŒì •ì´ ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤.")

    # Plot pitch tracks
    st.subheader("í”¼ì¹˜ íŠ¸ë˜í‚¹ (ì •ë‹µ vs í•™ìƒ)")
    st.markdown(
        "ì •ë‹µ: ì£¼í™©ìƒ‰ ì‹¤ì„  â€” MIDI(ë˜ëŠ” ìë™ ì¶”ì¶œëœ ê¸°ì¤€ ë©œë¡œë””)\n\n"
        "í•™ìƒ: íŒŒë€ìƒ‰ ì  â€” ì—…ë¡œë“œí•œ ë…¹ìŒì—ì„œ ì¶”ì¶œí•œ í”¼ì¹˜\n\n"
        "ë¹¨ê°„ Ã— í‘œì‹œëŠ” ì„¤ì •í•œ í—ˆìš© ë²”ìœ„(cent)ë¥¼ ë²—ì–´ë‚œ ì·¨ì•½ ìŒì • êµ¬ê°„ì…ë‹ˆë‹¤."
    )
    fig, ax = plt.subplots(figsize=(10, 3.5))

    # í•™ìƒ f0 (íŒŒë€ ì )
    ax.plot(student_times, student_f0, '.', markersize=3, color='tab:blue',
            alpha=0.8)  # ë ˆì „ë“œ ì œê±°: label ì‚¬ìš© ì•ˆí•¨

    # ì •ë‹µ f0 (ì£¼í™© ì‹¤ì„ )
    if ref_f0 is not None:
        ax.plot(ref_times, ref_f0, '-', linewidth=1.5, color='tab:orange',
                alpha=0.95)  # ë ˆì „ë“œ ì œê±°

    # ì·¨ì•½ ìŒì • í‘œì‹œ (ë¹¨ê°„ Ã—)
    if pitch_details:
        bad = [d for d in pitch_details if not d[2]]
        if bad:
            times_bad = [d[0] for d in bad]
            y_bad = np.interp(times_bad, student_times, np.nan_to_num(student_f0, nan=0.0))
            ax.scatter(times_bad, y_bad, color='red', marker='x', s=40)

    ax.set_xlabel("ì‹œê°„ (s)")

    # Yì¶•: Hz ëŒ€ì‹  ìŒì´ë¦„(C4 ë“±)ìœ¼ë¡œ í‘œì‹œ
    def midi_to_name(m):
        names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        return f"{names[int(m) % 12]}{int(m) // 12 - 1}"

    # í•™ìƒ/ì •ë‹µ ë°ì´í„°ì—ì„œ ìœ íš¨í•œ ì£¼íŒŒìˆ˜ ì¶”ì¶œ (0 ë˜ëŠ” NaN ì œì™¸)
    def valid_hz(arr):
        if arr is None:
            return np.array([])
        a = np.array(arr, dtype=float)
        a = a[~np.isnan(a)]
        a = a[a > 0]
        return a

    hz_student = valid_hz(student_f0)
    hz_ref = valid_hz(ref_f0) if 'ref_f0' in locals() and ref_f0 is not None else np.array([])
    all_hz = np.concatenate([hz_student, hz_ref]) if len(hz_student) + len(hz_ref) > 0 else np.array([440.0])

    min_hz = max(50.0, np.nanmin(all_hz) * 0.9)
    max_hz = min(20000.0, np.nanmax(all_hz) * 1.1)

    # MIDI ë²”ìœ„ (ì†Œìˆ˜ì  í¬í•¨)ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì ì ˆí•œ C(ì˜¥íƒ€ë¸Œ) ìœ„ì¹˜ë¥¼ ì„ íƒ
    midi_min = int(np.floor(librosa.hz_to_midi(min_hz)))
    midi_max = int(np.ceil(librosa.hz_to_midi(max_hz)))
    # C ìŒ(í˜ë‹¬ ê¸°ì¤€)ë§Œ í‘œì‹œ: MIDI ë²ˆí˜¸ % 12 == 0 ì¸ ê°’ë“¤
    c_notes = [n for n in range(midi_min - 1, midi_max + 1) if n % 12 == 0]
    if not c_notes:
        c_notes = list(range(midi_min, midi_max + 1, 12))

    hz_ticks = librosa.midi_to_hz(np.array(c_notes))
    labels = [midi_to_name(n) for n in c_notes]

    # ì•ˆì „ ì¥ì¹˜: tick ìœ„ì¹˜ê°€ y ë²”ìœ„ì— ë“¤ì–´ì˜¤ë„ë¡ ì œí•œ
    hz_ticks_in_range = [h for h in hz_ticks if (h >= min_hz and h <= max_hz)]
    labels_in_range = [labels[i] for i, h in enumerate(hz_ticks) if (h >= min_hz and h <= max_hz)]

    if hz_ticks_in_range:
        ax.set_yticks(hz_ticks_in_range)
        ax.set_yticklabels(labels_in_range)
    else:
        # fallback: ê¸°ë³¸ Hz ëˆˆê¸ˆ ìœ ì§€í•˜ë˜ ì†Œìˆ˜ ìë¦¿ìˆ˜ ì¤„ì„
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f"{x:.0f}"))

    ax.set_ylim(min_hz, max_hz)
    ax.grid(axis='x', linestyle=':', alpha=0.3)

    # ë ˆì „ë“œ(ì˜¤ë¥¸ìª½ ìƒë‹¨ í‘œì‹œ) ì œê±° â€” ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ë¹ˆ ê³µê°„ìœ¼ë¡œ ë‘ 
    # ax.legend(...) í˜¸ì¶œí•˜ì§€ ì•ŠìŒ

    st.pyplot(fig)

    # ì„¤ëª… í…ìŠ¤íŠ¸ (ëª…í™•í•˜ê²Œ ì–´ë–¤ ì„ /ì ì´ ì–´ë–¤ ë°ì´í„°ì¸ì§€)
    st.markdown(
        "- íŒŒë€ ì : í•™ìƒ ë…¹ìŒì—ì„œ ì¶”ì¶œí•œ ì‹¤ì œ ë°œì„± í”¼ì¹˜\n"
        "- ì£¼í™© ì‹¤ì„ : êµì‚¬ê°€ ì œê³µí•œ MIDI(ë˜ëŠ” ìë™ ì¶”ì¶œí•œ ê¸°ì¤€ ë©œë¡œë””)\n"
        "- ë¹¨ê°„ Ã—: í•™ìƒ í”¼ì¹˜ê°€ ì„¤ì •í•œ cent í—ˆìš© ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ êµ¬ê°„\n"
    )

    # Show onset comparison plot
    st.subheader("ì˜¨ì…‹(ë°•ì) ë¹„êµ")
    st.markdown(
        "íŒŒí˜• ìœ„ì˜ ì„¸ë¡œì„ ìœ¼ë¡œ ë°•ì(ì˜¨ì…‹)ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n\n"
        "- íŒŒë€ ì„¸ë¡œì„ : í•™ìƒì˜ ë…¹ìŒì—ì„œ ê°ì§€ëœ ì˜¨ì…‹\n"
        "- ì´ˆë¡ ì ì„ : ì •ë‹µ MIDI(ë˜ëŠ” ìë™ ì¶”ì¶œ) ì˜¨ì…‹\n"
        "ì‹œê°„ ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ë°•ì ì •ë‹µìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤."
    )
    fig2, ax2 = plt.subplots(figsize=(10, 1.6))
    # draw audio waveform
    librosa.display.waveshow(y, sr=sr, alpha=0.45, ax=ax2)
    # plot detected onsets (í•™ìƒ)
    ax2.vlines(detected_onsets, -1, 1, color='tab:blue', alpha=0.9, label='í•™ìƒ ì˜¨ì…‹ (ê°ì§€)')
    # plot reference onsets (ì •ë‹µ)
    if len(ref_onsets) > 0:
        ax2.vlines(ref_onsets, -1, 1, color='tab:green', alpha=0.9, linestyle='dashed',
                   label='ì •ë‹µ ì˜¨ì…‹ (MIDI/ê¸°ì¤€)')
    ax2.legend(loc='upper right', ncol=2, fontsize='small')
    ax2.set_xlim(0, len(y) / sr)
    ax2.set_ylim(-1.1, 1.1)
    ax2.set_yticks([])
    ax2.set_xlabel("ì‹œê°„ (s)")
    st.pyplot(fig2)

    st.success("ë¶„ì„ ì™„ë£Œ")
    st.caption("ì°¸ê³ : ê·¸ë˜í”„ì˜ ìƒ‰ìƒ/í‘œì‹ì€ ìœ„ ì„¤ëª…ì„ ë”°ë¦…ë‹ˆë‹¤. ì†ŒìŒ/ë‹¤ì¤‘ ì„±ë¶€ê°€ ìˆìœ¼ë©´ ìë™ ì¶”ì¶œ ê¸°ì¤€ê³¼ ì°¨ì´ê°€ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
